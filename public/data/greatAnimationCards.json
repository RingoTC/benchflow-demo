[
  {
    "title": "WebArena",
    "description": "A benchmark for evaluating LLMs on web-based interaction tasks, testing their ability to navigate and interact with real-world websites.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "UCLA"
    },
    "benchmarkUrl": "https://webarena.dev"
  },
  {
    "title": "MLE-Bench",
    "description": "A benchmark for measuring how well AI agents perform at machine learning engineering tasks.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Microsoft Research"
    },
    "benchmarkUrl": "https://github.com/microsoft/mle-bench"
  },
  {
    "title": "SWE-bench",
    "description": "A benchmark for evaluating LLMs on real-world software engineering tasks from GitHub pull requests.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Carnegie Mellon University"
    },
    "benchmarkUrl": "https://github.com/princeton-nlp/SWE-bench",
    "highlight": true
  },
  {
    "title": "SWE-bench Multimodal",
    "description": "An extension of SWE-bench that includes multimodal software engineering tasks involving code, images, and documentation.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Carnegie Mellon University"
    },
    "benchmarkUrl": "https://github.com/princeton-nlp/SWE-bench"
  },
  {
    "title": "AgentBench",
    "description": "A comprehensive benchmark for evaluating LLM-powered agents across various real-world scenarios and tasks.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Tsinghua University"
    },
    "benchmarkUrl": "https://github.com/THUDM/AgentBench",
    "highlight": true
  },
  {
    "title": "Tau (ùúè)-Bench",
    "description": "A benchmark focusing on evaluating language models' tool-use capabilities across diverse scenarios.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Microsoft Research"
    },
    "benchmarkUrl": "https://github.com/microsoft/tau-bench",
    "highlight": true
  },
  {
    "title": "BIRD-SQL",
    "description": "A large-scale cross-domain SQL benchmark for testing text-to-SQL capabilities of language models.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "ByteDance AI Lab"
    },
    "benchmarkUrl": "https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird"
  },
  {
    "title": "LegalBench",
    "description": "A comprehensive benchmark for evaluating legal reasoning and understanding capabilities of language models.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Stanford Law School"
    },
    "benchmarkUrl": "https://github.com/HazyResearch/legalbench"
  },
  {
    "title": "GLUE",
    "description": "General Language Understanding Evaluation benchmark, a collection of diverse natural language understanding tasks.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "NYU & UW & DeepMind"
    },
    "benchmarkUrl": "https://gluebenchmark.com"
  },
  {
    "title": "MS MARCO",
    "description": "A large-scale dataset for machine reading comprehension and question answering.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Microsoft Research"
    },
    "benchmarkUrl": "https://microsoft.github.io/msmarco/"
  },
  {
    "title": "Stanford HELM",
    "description": "Holistic Evaluation of Language Models - a framework for rigorous and comprehensive language model evaluation.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Stanford University"
    },
    "benchmarkUrl": "https://crfm.stanford.edu/helm"
  },
  {
    "title": "API-Bank",
    "description": "A benchmark for evaluating language models' ability to understand and use various APIs effectively.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Microsoft Research"
    },
    "benchmarkUrl": "https://github.com/microsoft/api-bank"
  },
  {
    "title": "ARC",
    "description": "AI2 Reasoning Challenge - a question answering dataset testing reasoning capabilities.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Allen Institute for AI"
    },
    "benchmarkUrl": "https://allenai.org/data/arc"
  },
  {
    "title": "HellaSwag",
    "description": "A challenge dataset for evaluating commonsense inference in models.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "University of Washington"
    },
    "benchmarkUrl": "https://rowanzellers.com/hellaswag"
  },
  {
    "title": "HumanEval",
    "description": "A benchmark for evaluating language models on Python programming tasks.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "OpenAI"
    },
    "benchmarkUrl": "https://github.com/openai/human-eval"
  },
  {
    "title": "MMLU",
    "description": "Massive Multitask Language Understanding, testing knowledge across 57 subjects.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "EleutherAI"
    },
    "benchmarkUrl": "https://github.com/hendrycks/test"
  },
  {
    "title": "SuperGLUE",
    "description": "A more difficult extension of GLUE with more challenging language understanding tasks.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "NYU & UW & DeepMind"
    },
    "benchmarkUrl": "https://super.gluebenchmark.com"
  },
  {
    "title": "WebArena",
    "description": "A benchmark for evaluating LLMs on web-based interaction tasks, testing their ability to navigate and interact with real-world websites.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "UCLA"
    },
    "benchmarkUrl": "https://webarena.dev"
  },
  {
    "title": "MLE-Bench",
    "description": "A benchmark for measuring how well AI agents perform at machine learning engineering tasks.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Microsoft Research"
    },
    "benchmarkUrl": "https://github.com/microsoft/mle-bench"
  },
  {
    "title": "SWE-bench",
    "description": "A benchmark for evaluating LLMs on real-world software engineering tasks from GitHub pull requests.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Carnegie Mellon University"
    },
    "benchmarkUrl": "https://github.com/princeton-nlp/SWE-bench"
  },
  {
    "title": "SWE-bench Multimodal",
    "description": "An extension of SWE-bench that includes multimodal software engineering tasks involving code, images, and documentation.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Carnegie Mellon University"
    },
    "benchmarkUrl": "https://github.com/princeton-nlp/SWE-bench"
  },
  {
    "title": "AgentBench",
    "description": "A comprehensive benchmark for evaluating LLM-powered agents across various real-world scenarios and tasks.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Tsinghua University"
    },
    "benchmarkUrl": "https://github.com/THUDM/AgentBench"
  },
  {
    "title": "Tau (ùúè)-Bench",
    "description": "A benchmark focusing on evaluating language models' tool-use capabilities across diverse scenarios.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Microsoft Research"
    },
    "benchmarkUrl": "https://github.com/microsoft/tau-bench"
  },
  {
    "title": "BIRD-SQL",
    "description": "A large-scale cross-domain SQL benchmark for testing text-to-SQL capabilities of language models.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "ByteDance AI Lab"
    },
    "benchmarkUrl": "https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird"
  },
  {
    "title": "LegalBench",
    "description": "A comprehensive benchmark for evaluating legal reasoning and understanding capabilities of language models.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Stanford Law School"
    },
    "benchmarkUrl": "https://github.com/HazyResearch/legalbench"
  },
  {
    "title": "GLUE",
    "description": "General Language Understanding Evaluation benchmark, a collection of diverse natural language understanding tasks.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "NYU & UW & DeepMind"
    },
    "benchmarkUrl": "https://gluebenchmark.com"
  },
  {
    "title": "MS MARCO",
    "description": "A large-scale dataset for machine reading comprehension and question answering.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Microsoft Research"
    },
    "benchmarkUrl": "https://microsoft.github.io/msmarco/"
  },
  {
    "title": "Stanford HELM",
    "description": "Holistic Evaluation of Language Models - a framework for rigorous and comprehensive language model evaluation.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Stanford University"
    },
    "benchmarkUrl": "https://crfm.stanford.edu/helm"
  },
  {
    "title": "API-Bank",
    "description": "A benchmark for evaluating language models' ability to understand and use various APIs effectively.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Microsoft Research"
    },
    "benchmarkUrl": "https://github.com/microsoft/api-bank"
  },
  {
    "title": "ARC",
    "description": "AI2 Reasoning Challenge - a question answering dataset testing reasoning capabilities.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "Allen Institute for AI"
    },
    "benchmarkUrl": "https://allenai.org/data/arc",
    "highlight": true
  },
  {
    "title": "HellaSwag",
    "description": "A challenge dataset for evaluating commonsense inference in models.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "University of Washington"
    },
    "benchmarkUrl": "https://rowanzellers.com/hellaswag"
  },
  {
    "title": "HumanEval",
    "description": "A benchmark for evaluating language models on Python programming tasks.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "OpenAI"
    },
    "benchmarkUrl": "https://github.com/openai/human-eval"
  },
  {
    "title": "MMLU",
    "description": "Massive Multitask Language Understanding, testing knowledge across 57 subjects.",
    "organization": {
      "icon": "/data/logo/OpenAI.png",
      "name": "EleutherAI"
    },
    "benchmarkUrl": "https://github.com/hendrycks/test"
  }
]